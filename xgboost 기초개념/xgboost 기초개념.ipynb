{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그림, 실습코드 등 학습자료 출처 : https://brunch.co.kr/@snobberys/137\n",
    "\n",
    "#### 1. xgboost 개념이해를 위한 Boosting algorithm 기본원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.jpg\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. xgboost 기본원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2.jpg\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 기본원리를 수식으로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"3.jpg\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"4.jpg\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"5.jpg\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6.jpg\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Parameter\n",
    "\n",
    "xgboost의 파라미터는 크게 3가지 종류가 있다.\n",
    "\n",
    "1) 도구의 모양을 결정하는 'general parameters'\n",
    "\n",
    "1-1) booster : 어떤 부스터 구조를 쓸지 결정한다. gbtree, gblinear, dart가 있다.\n",
    "\n",
    "1-2) nthread : 몇개의 쓰레드를 동시에 처리하도록 할지 결정한다. 디폴트 옵션은 \"최대한 많이\"다\n",
    "\n",
    "1-3) num_feature : 피쳐 차원의 숫자를 정해야 하는 경우 이 옵션을 세팅한다. 디폴트 옵션은 \"최대한 많이\"다\n",
    "\n",
    "\n",
    "2) 트리마다 가지를 칠때 적용하는 옵션을 정의하는 'booster parameters'\n",
    "\n",
    "2-1) eta : learning rate다. 트리에 가지가 많을수록 오버피팅하기 쉽다. 매 부스팅 스탭마다 weight를 주어 부스팅 과정에 과적합이 일어나지 않도록 한다.\n",
    "\n",
    "2-2) gamma : information gain에서 -r로 표현한것이 있는데 그게 커지면 트리 깊이가 줄어들어 보수적인 모델이 된다. 디폴트 값은 0이다.\n",
    "\n",
    "2-3) max_depth : 한 트리의 maximum depth이다. 숫자가 커질수록 모델의 복잡도가 커진다. 다시말해서 과적합 하기 쉽다. 디폴트는 6이고 이때 리프노드의 개수는 2의 6승 64개이다.\n",
    "\n",
    "2-4) lambda(L2 reg-form) : L2 regularization form에 달리는 weights이다. 숫자가 클수록 보수적이 모델이 된다.\n",
    "\n",
    "2-5) alpha(L1 reg-form) : L1 regularization form weights다. 숫자가 클수록 보수적인 모델이된다.\n",
    "\n",
    "3) 학습과정에서 최적의 퍼포먼스를 결정하는 'learning task parameters'\n",
    "\n",
    "3-1) objective : 목적함수다. reg:linear(linear-regression), binary:logistic(binary-logistic classification), count:poisson(count data poison regression) 등 다양하다.\n",
    "\n",
    "3-2) eval_metric : 모델의 평가 함수를 조정하는 함수다. rmse(root mean square error), logloss(log-likelihood), map(mean average precision) 등, 해당데이터의 특성에 맞게 평가함수를 조정한다.\n",
    "\n",
    "4) command line parameters\n",
    "\n",
    "4-1) num_rounds : 부스팅 라운드를 결정한다. 랜덤하게 생성되는 모델이기 때문에 이 수가 적당히 큰게 좋다. epoch 옵션과 유사하다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
