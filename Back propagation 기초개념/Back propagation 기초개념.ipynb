{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그림, 실습코드 등 학습자료 출처 : https://www.fastcampus.co.kr/data_online_databasic\n",
    "\n",
    "우리가 딥러닝에서 weight를 구할때 gradient descent로 미분을 한다는건 알겠는데 그러면 어떻게 할것인가에 대해 아래 그림을 보면서 생각을 해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 결국에는 z2를 잘 구해서 최대한 정답과 같아야하는게 목표다.\n",
    "\n",
    "z11 = x1 * w11 + x2 * w12 + x3 * w13 + x4 * w14\n",
    "\n",
    "$\\  \\begin{eqnarray} \n",
    "a11 = \\sigma(z11) = \\frac{1}{1+e^{-z11}} \n",
    "\\end{eqnarray} $\n",
    "\n",
    "z2 = a11 * w21 + a12 * w22 + a13 * w23\n",
    "\n",
    "$\\  \\begin{eqnarray} \n",
    "a2 = \\sigma(z2) = \\frac{1}{1+e^{-z2}} \n",
    "\\end{eqnarray} $\n",
    "\n",
    "$\\ L = (y - a2)^2 $ 일때\n",
    "\n",
    "이때 w11을 update하기 위해 L(Loss)를 w11로 미분을해야 하는데 이거 식이 복잡해서 어떻게 하냐! 라는 문제가 나온다.\n",
    "\n",
    "Loss부터 거꾸로 한단계씩 미분을 해보자\n",
    "\n",
    "step1) Loss를 a2로 미분한다.\n",
    "\n",
    "$\\ \\begin{eqnarray}  \n",
    "\\frac{\\partial L}{\\partial a2} =  -2(y - a2)\n",
    "\\end{eqnarray} $\n",
    "\n",
    "step2) a2를 z2로 미분한다.\n",
    "\n",
    "$\\ \\begin{eqnarray}  \n",
    "\\frac{\\partial a2}{\\partial z2} =  1\n",
    "\\end{eqnarray} $\n",
    "\n",
    "step3) z2를 a11로 미분한다.\n",
    "\n",
    "$\\ \\begin{eqnarray}  \n",
    "\\frac{\\partial z2}{\\partial a11} =  w21\n",
    "\\end{eqnarray} $\n",
    "\n",
    "step4) a11를 z11로 미분한다.\n",
    "\n",
    "$\\ \\begin{eqnarray}  \n",
    "\\frac{\\partial a11}{\\partial z11} =  \\sigma(z11) * (1-\\sigma(z11))\n",
    "\\end{eqnarray} $\n",
    "\n",
    "step5) z11를 w11로 미분한다.\n",
    "\n",
    "$\\ \\begin{eqnarray}  \n",
    "\\frac{\\partial a11}{\\partial z11} =  x1\n",
    "\\end{eqnarray} $\n",
    "\n",
    "step6) 위와같이 다섯단계의 미분을 cahin rule이라는 규칙에 의해서 전부 각각 곱하면 아래와 같다.\n",
    "\n",
    "$\\ \\begin{eqnarray}  \n",
    "\\frac{\\partial L}{\\partial a2} * \\frac{\\partial a2}{\\partial z2} * \\frac{\\partial z2}{\\partial a11} * \\frac{\\partial a11}{\\partial z11} * \\frac{\\partial z11}{\\partial w11} = \\frac{\\partial L}{\\partial w11}\n",
    "\\end{eqnarray} $\n",
    "\n",
    "이런식으로 한단계 한단계 거꾸로 미분값들을 구해서 chain rule에 의해 곱해나가는 것을 back propagation이라고 한다.\n",
    "\n",
    "정리하면 Loss로부터 거꾸로 한단계씩 미분값을 구하고 이 값들을 chain rule에 의하여 곱해가면서 weight에 대한 gradient를 구하는 방법이다.\n",
    "\n",
    "Loss로 부터 하나하나 거꾸로 스텝을 밟아가면서 처음에 데이터가 지나갔던 경로의 반대방향으로 weight에 대한 미분값을 곱해나가는 방법이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
